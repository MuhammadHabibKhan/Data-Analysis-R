---
title: "B20102088-MuhammadHabibKhan-FinalLab"
author: "Muhammad Habib Khan"
date: "1/22/2022"
output:
  word_document:
    toc: yes
    fig_caption: yes
  pdf_document:
    toc: yes
---

```{r setup, include=TRUE}

# Converting Time Series into Data Frame Structure

data(Seatbelts)
Seatbelts <- data.frame(Year=floor(time(Seatbelts)),
Month=factor(cycle(Seatbelts),
labels=month.abb), Seatbelts)

knitr::opts_chunk$set(echo = TRUE)

library(knitr)
```

# Selecting a Dataset

Data Set Name: *Seatbelts*

Data Set From built-in R Data sets in package ‘datasets’

**Abstract**: Road Casualties in Great Britain 1969-84

**Data Set Information:**

The ‘Seatbelts’ data set in R is a multiple time-series data set that was 
commissioned by the Department of Transport in 1984 to measure differences in 
deaths before and after front seat belt legislation was introduced on 31st 
January 1983. It provides monthly total numerical data on a number of incidents 
including those related to death and injury in Road Traffic Accidents (RTA’s). 
The data set starts in January 1969 and observations run until December 1984.

**Attributes:**

DriversKilled:    Car drivers killed.

drivers:          Same as *UKDriverDeaths* deaths count.

*UKDriverDeaths is a time series giving the monthly totals of car drivers in Great Britain killed or seriously injured Jan 1969 to Dec 1984. Compulsory wearing of seat belts was introduced on 31 Jan 1983.*
*Seat belts is more information on the same problem.*

front:           Front-seat passengers killed or seriously injured.

rear:            Rear-seat passengers killed or seriously injured.

kms:             Distance driven.

PetrolPrice:     Petrol price.

VanKilled:       Number of van (‘light goods vehicle’) drivers killed.

law:             0/1: Was the law in effect that month?

# Data Set At A Glance

```{r}
# Summary of the Entire Data
summary(Seatbelts)

# Gist of the Data including only few of the rows
head(Seatbelts, 5)

# Another useful command to look at the summarized data and skim it

library(skimr)
skim(Seatbelts)

# Another command to displays the type and a preview of all columns as a row 
# so that it's very easy to take in and easily preview data type and sample data.

library(dplyr)
glimpse(Seatbelts)

# A comprehensive look at the entire data set produced in html format 
# It includes a ton of information including the basic statistics, structure, 
# missing data, distribution visualizations, correlation matrix and principal 
# component analysis. The screenshots of the webpage are attached below and the
# html file is also attached. 

library(DataExplorer)
DataExplorer::create_report(datasets::Seatbelts)
```


# Basic Plots

Some basic plots, charts and graphs of the data set and its classes are shown 
below which includes:

1) Bar Plot
2) 3D Pie Chart
3) Histogram
4) Scatter Plot
5) Box Plot

## Bar Plot:

```{r}
attach(Seatbelts)

# Bar-plot of Number of Van Drivers killed through out the years

barplot(Seatbelts$VanKilled, xlab='Time (months)', ylab='Number of Van Drivers Killed', col=heat.colors(10))
```

The bar plot above shows the number of Van Drivers killed during a car accident in Britain in the described time period. While the numbers were small to begin with, we can see the entire pattern going down on a negative slope as they month passes, indicating that when the law of seat belts being mandatory was passed, the amount of deaths for van drivers decreased too even though the time was beginning of Industrial Revolution and the cars on the roads increased as the days passed.


## 3D Pie Chart:

```{r}

# A 3D pie chart of the mean of the number of car drivers, van drivers, front seat and rear seat passengers killed through out Jan, 1969 - Dec, 1984

library(plotrix)
x <- c(mean(Seatbelts$DriversKilled), mean(Seatbelts$front), mean(Seatbelts$rear), mean(Seatbelts$VanKilled))
names(x) <- c("DriversKilled", "front", "rear", "VanKilled")
pie3D(x, labels = names(x), col = cm.colors(4), main = "3DPieChart of Drivers,VanDrivers,Front & Rear seat passengers\nkilled or injured in Great Britain during  1969-1984 ", labelcol = "darkgreen", col.main = "darkgreen")

```

Above I have constructed a 3D pie chart using the plotrix library. The pie chart shows the proportion of casualties in car accidents. An average was taken of the classes through out the period to construct this chart. The least are van drivers. This must be because vans are usually less on the roads then sedan/coupe cars. Secondly, vans are often driven slowly then the other cars. The most portion is taken by the front seat passengers, followed by rear seat passengers and after that comes the drivers at 3rd place. Surprisingly, according to the data provided, passengers, front more than rear, are likely to meet the fate of death than the one behind the wheel. One reason may be the fact that most driver seats have airbags while the passengers seat do not. 


## Histogram:

```{r}
# Histogram for Drivers killed through out the period. Histogram is same as bar plot except that they depict the data in grouped form rather than discrete. 

hist(DriversKilled , col=terrain.colors(11) , border = 'blue', main='Histogram For Number of Car Drivers Killed\nduring 1969-1984 in The Great Britain\n ', xlab = 'Car Drivers Killed ' )

```

The histogram plotted demonstrates what was the usual toll of deaths of car drivers in the given time period. As we can see, at most times the casualties were between 100 and 120, peaking at 200 and hitting the lowest at 60. 


## Scatter Plot:

```{r}
# Scatter Plot Between Increment in Petrol Prices & Number of Drivers Killed through out the period 

plot(PetrolPrice, DriversKilled, type='p', xlab = 'Petrol Price', ylab='Number of Car Drivers Killed', main='Petrol Prices VS Car Drivers Deaths', col='magenta')

```

Above us we can see a scatter plot between increment in petrol prices and number of drivers killed through out the period. If we try to look at the relationship between the two classes, and try to predict a trend based on this plot, we may find that the death of car drivers loosely depend on petrol prices much as they declined slowly at each price point except for at the last one which is the highest price where we see a sudden drop. This may have to do with the introduction of law of seat belts introduced and being implemented by then and also the fact that car safety features were becoming more and more common with airbags and other durability features added to withstand such situations. A better petrol price vs deaths plot will be when we consider only the months when law was not passed which we will look into in the regression section. 


## Box Plot:

```{r}

# Box plot of people killed/seriously injured during car accidents in Great Britain during 1969-1984. The box plots represents five values in the graph i.e., minimum, first quartile, second quartile (median), third quartile, the maximum value of the data vector.

boxplot(drivers, col='orange', xlab = "Box Plot (Jan,1969 - Dec, 1984)", ylab = "People killed/seriously injured during car accidents", col.axis = "darkgreen", col.lab = "dark blue")

```

This box plot is based on the total amount of people killed or seriously injured during the given period. This is the same as the data set *UKDriverDeaths*. There are five values that this plot tells us. As we can see, the minimum amount of casualties was around 1000, then the first quartile is a little less than 1500, the median (second quartile; black line) is around 1700 while the third quartile is closing to 2000. The maximum here can be seen close to 2500.


# Correlation

**Definition:**
                When a variable tends to change from one to another, whether direct or indirect, it is considered correlated. A correlation coefficient is applied to measure a degree of association in variables and is usually called Pearson’s correlation coefficient. The correlation coefficient is measured on a scale with values from +1 through 0 and -1. When both variables increase, the correlation is positive, and if one variable increases, and the other decreases, the correlation is negative.

## Correlation between Law & Casualties:

Lets have a look at the most important correlation of the data set:

```{r}

plot(law, drivers, pch = 19, col = "pink")
abline(lm(drivers ~ law), col = "red", lwd = 3)
text(paste("Correlation:", round(cor(law, drivers), 2)), x = 0.4, y = 2300)
```
The above scatter plot depicts the correlation between the total amount of people killed or seriously injured during the given period and if the law was passed then or not (0 = not passed, 1 = passed). It is clearly evident that the casualties decreases by a great deal after the law was passed. The *cor* commands tells us that the correlation coefficient is negative 0.45 (-0.45) which proves that deaths decreases as law is passed. The r-value in magnitude is 0.45 which tells that this is a fairly moderate correlation. If we were to consider other factors too, which includes the increased number of vehicles on the road or the increased population, then this correlation is pretty good even without those factors included. But to counter those extra factors, the safety of the vehicles increased/improved too. 


## Correlation of the Entire Dataset:

```{r}
graphics.off()
library(corrplot)

# converting months into numeric data as cor command does not accept non-numeric data
Seatbelts$Month <- as.numeric(Month) 

# This command prints the correlation between all of the classes of the data set
cor(Seatbelts)

# Converting into matrix to use with command *corrplot* 
cor.mat.seatbelts = cor(Seatbelts)

# Plotting the correlation between all of the classes of the data set visually
corrplot(cor.mat.seatbelts)

# Another way of displaying correlation plot between all the classes, almost the same as corrplot command but this one displays the Pearson's Coefficient written in numbers rather than fading colors
library(psych)
corPlot(cor.mat.seatbelts, cex = 1.2)
```

The cor command prints the correlation between all of the classes of the data set numerically in a tabular form. Using corrplot, we plot all that data into a visually appealing and easy to grasp graph. Some main selective insights from this correlation data:

* The petrol prices and years are moderately positively correlated, meaning that the petrol prices raised as the years passes

* Years passed and law passed are weakly but close to moderately correlated with the total amount of people killed or seriously injured in car accidents. However, they both are moderately correlated with the deaths of front seat passengers. The correlation is negative in all instances showing that years passing resulted in less deaths.

* Years passed and number of Van Drivers killed also has a moderate negative correlation.

* The kms (distance) also has a weak but close to moderate negative correlation with the total amount of people killed or seriously injured in car accidents. 


# Confidence Interval

**Definition:**
              Confidence intervals (CI) are part of inferential statistics that help in making inference about a population from a sample. Based on the confidence level, a true population mean is likely covered by a range of values called confidence interval.
              
Here, I have constructed two confidence intervals. One on *VanDrivers* and second on *front* based on z-distribution and t-distribution respectively.

## Based on Z-Distribution

```{r}

library(Rmisc)

# This function computes Confidence intervals based on Z-Distribution
CI_z <- function (x, ci = 0.95)
{
`%>%` <- magrittr::`%>%`
standard_deviation <- sd(x)
sample_size <- length(x)
Margin_Error <- abs(qnorm((1-ci)/2))* standard_deviation/sqrt(sample_size)
df_out <- data.frame( sample_size=length(x), Mean=mean(x), sd=sd(x),
Margin_Error=Margin_Error,
'CI lower limit'=(mean(x) - Margin_Error),
'CI Upper limit'=(mean(x) + Margin_Error)) %>%
tidyr::pivot_longer(names_to = "Measurements", values_to ="values", 1:6 )
return(df_out)
}


# Selecting a random sample of 30 from VanKilled (no. of van drivers killed)
v_killed = sample(VanKilled, 30)

# Checking normality to apply z-distribution or not
shapiro.test(v_killed)

# As p > 0.05 in Shapiro normality test , hence we can apply z-distribution and the sample size is also 30.
CI_z(v_killed, ci= 0.95)

# Population mean of VanKilled
mean(VanKilled)

# Confidence Interval Plot
library(plotrix)
plotCI(v_killed, li = 7.921209	 , ui = 10.278791 )

# Density Plot showing the VanKilled Distribution
plot(density(v_killed, width=10), col = 'brown', main= 'Density Plot of Random Sample from Van\n Drivers Killed')

```

**Explanation:** Using the sample command, we select a random sample of size 30 (n=30) from the VanKilled class. Then using Shapiro test, we test for normality and as the p value is found to be greater than 0.05, the normality checks out. Because the sample size is 30, we opt for z-distribution. The function calculates the upper and lower limit and our population mean *9.057292* falls in the range. As the interval is set at 0.95 hence there is a 95% chance of the population mean to be in between this range every time the result is calculated and 5% chance of falling out of the range.

**Graphs:**
Using the upper and lower limits, we plot the Confidence Interval Scatter Plot showing the interval.
Secondly, a density plot of the distribution of VanKilled is displayed, showing its normal nature. 


## Based on t-Distribution

```{r}

# This function computes Confidence intervals based on t-Distribution
CI_t <- function (x, ci = 0.95)
{
`%>%` <- magrittr::`%>%`
Margin_Error <- qt(ci + (1 - ci)/2, df = length(x) - 1) * sd(x)/sqrt(length(x))
df_out <- data.frame( sample_size=length(x), Mean=mean(x), sd=sd(x),
Margin_Error=Margin_Error,
'CI lower limit'=(mean(x) - Margin_Error),
'CI Upper limit'=(mean(x) + Margin_Error)) %>%
tidyr::pivot_longer(names_to = "Measurements", values_to ="values", 1:6 )
return(df_out)
}

# Selecting a random sample of 30 from VanKilled (no. of van drivers killed)
f_killed = sample(front, 20)

# Checking normality 
shapiro.test(f_killed)

# As the p > 0.05 in Shapiro normality test , hence we can apply t-distribution as the sample size is also less than 30.
CI_t(f_killed, ci= 0.95)

# Population mean of VanKilled
mean(front)

# Confidence Interval Plot
library(plotrix)
plotCI(f_killed, li = 734.24234		 , ui = 926.85766 )

# Density Plot showing the 'front' Distribution
plot(density(f_killed), col = 'purple', main= 'Density Plot of Random Sample from\nFront Seat Passengers Killed')

```

**Explanation:** Using the sample command, we select a random sample of size 20 (n=20) from the VanKilled class. Then using Shapiro test, we test for normality and as the p value is found to be greater than 0.05, the normality checks out. Because the sample size is 20 (less than 30), we opt for t-distribution. The function calculates the upper and lower limit and our population mean *837.2188* falls in the range. As the interval is set at 0.95 hence there is a 95% chance of the population mean to be in between this range every time the result is calculated and 5% chance of falling out of the range.

**Graphs:**
Using the upper and lower limits, we plot the Confidence Interval Scatter Plot showing the interval.
Secondly, a density plot of the distribution of *front* is displayed, showing its normal nature. 


# Hypothesis Testing

**Definition:**
             A hypothesis is made by the researchers about the data collected for any experiment or data set. A hypothesis is an assumption made by the researchers that are not mandatory true. In simple words, a hypothesis is a decision taken by the researchers based on the data of the population collected.

## One Sample t-test:

```{r}

library(stats)
# taking a sample of total deaths and serious injuries during car accidents in the given period
tdrivers = sample(drivers, 20)

# Shapiro Test to check the normality
shapiro.test(tdrivers)

# using t test command to get the hypothesis test with Ho = mean equals 1500 and H1 = mean not equals 1500
t_test_drivers <- t.test(tdrivers, mu = 1500)
t_test_drivers

# Using the *webr* package we plot the one sample t-test
require(webr)
plot(t_test_drivers)

```

**Explanation:** For this one sample hypothesis test using t-distribution, we take a random sample of size 20 from the class *drivers*. Using Shapiro test, we confirm the normality as to whether it is right to use t-distribution here and as the p-value was > 0.05, hence normality is true. Then using the t-test command from the library *stats* we generate a hypothesis test against the sample with;

Ho (null hypothesis) = mean equals 1500
H1 (alternative hypothesis) = mean not equals 1500

The P-Value from this test is found to be *0.01523*. If we were to make a decision whether to reject or accept null hypothesis on the basis of level of significance (alpha) = 0.05 (5%), then our null hypothesis will be **rejected here** here as the alpha-value > p-value & we accept the **alternative hypothesis**

Using the *webr* package, we plot the one-sample t-test which shows us that our value resides in the critical region hence accepting the alternative hypothesis.


## Two sample t-test for two independent samples

```{r}

library(stats)
# taking a sample of 20 from class *front* (front seat passenger casualties)
tfront = sample(front, 20)

# taking a sample of 20 from class *rear* (rear seat passenger casualties)
trear = sample(rear, 20)

# Shapiro Test to check the normality
shapiro.test(tfront)
shapiro.test(trear)

# using t test command to get the hypothesis test with Ho = mean 1 equals mean 2 and H1 = mean 1 not equals mean 2
t_test_passengers <- t.test(tfront, trear)
t_test_passengers

# Using the *webr* package we plot the two sample t-test
require(webr)
plot(t_test_passengers)

```
**Explanation:** For this two sample hypothesis test using t-distribution, we take a random sample of size 20 from the class *front* and *rear* and hope to see if the average number of deaths of rear and front seat passengers are the same. 
Using Shapiro test, we confirm the normality as to whether it is right to use t-distribution here and as the p-value was > 0.05 for both the random samples, hence normality is true. 
Then using the t-test command from the library *stats* we generate a hypothesis test against the samples with;

Ho (null hypothesis) = mean 1 equals mean 2 (front avg. = rear avg.)
H1 (alternative hypothesis) = mean 1 not equals mean 2 (front avg. != rear avg.)

The P-Value from this test is found to be *0.0000000001764*. If we were to make a decision whether to reject or accept null hypothesis on the basis of level of significance (alpha) = 0.01 (1%), then our null hypothesis will be **rejected here** here as the alpha-value > p-value & we accept the **alternative hypothesis** 

The t-test tells that the mean of front is *848.20* and mean of rear is *409.05* and our p-value also confirm that there's a highly significant and overwhelming evidence that both the means are not same. This consequently proves that the front seat casualties are higher than the rear seat deaths and statistically, one is more likely to die while sitting at the front during a car accident.

Using the *webr* package, we plot the two-sample t-test which shows us that our value does not reside in the acceptable region hence accepting the alternative hypothesis and rejecting null.


# Chi-Square Test

Chi-Square test in R is a statistical method which used to determine if two categorical variables have a significant correlation between them. The two variables are selected from the same population. 

## Chi-Square Test of Independence:

```{r}
# Chi-Square test
chsq_test = chisq.test(Month, drivers, correct=TRUE)
chsq_test

# Using the *webr* package we plot the chi-square test of independence
require(webr)
plot(chsq_test)

```

**Explanation:** We run the chi-square test of independence on *Months* and *drivers* (total people injured/seriously injured during accidents). Here the hypothesis is;

Ho = Months and Drivers are independent 
H1 = Months and Drivers are dependent 

After running the test using *chisq.test* command, we find the p-value to be *0.1523*. If we choose the level of significance to be 5% (alpha = 0.05), then p-value > alpha and we must reject the null hypothesis and **accept the alternative hypothesis** that *Months* and *drivers* are related.

Plotting the chi-square distribution using *webr* package, we can clearly see that the calculated value lies long before the critical region and hence the we accept alternative hypothesis of dependency between two variables.


# ANOVA

Like the t-test, Analysis of Variance (ANOVA) helps you find out whether the differences between groups of data are statistically significant. Difference is t-test is limited for two groups only while ANOVA can be applied on more than two groups. It works by analyzing the levels of variance within the groups through samples taken from each of them.

## One-Way ANOVA

```{r}
# First we use aov() to run the model
a_oneway <- aov(drivers ~ law)

# then we use summary() to print the summary of the model
summary(a_oneway)

# Box plot
boxplot(drivers ~ law, col = heat.colors(2))

# Multiple plots from webr
plot(a_oneway)


```

**Explanation:** Here we test the total deaths and serious injuries in car accidents (drivers: quantitative variable) against the categorical variable law passed or not (passed:1, not passed:0). Here the hypothesis is;

Null hypothesis (H0) : The average mean of two groups are not different.
Alternative hypothesis (H1): The average mean of two groups are different.

Using the aov and summary command, we find that p-value is 9.76e-11. If we test it against the significance level of 5% (alpha = 0.05) then as p-value < alpha, we **reject the null hypothesis** in favor of alternative hypothesis. Hence the mean of two groups are different which proves that less people died when law of Seat belts was passed. 

The box plot of both classes also shows that the mean value of both groups differ significantly. 

Using webr package, we can plots and get more insight on the data. We can see from Q-Q plot that the data is overall fairly normally distributed. The fitted vs Residual plots shows the difference between the expected and actual values.

## Two-Way ANOVA:

```{r}

# First we use aov() to run the model
a_twoway <- aov(drivers ~ law+Month)

# then we use summary() to print the summary of the model
summary(a_twoway)

# Box plot
boxplot(drivers ~ law+Month, col = topo.colors(2))

# Multiple plots from webr
plot(a_twoway)
```

**Explanation:** Here we test the total deaths and serious injuries in car accidents (drivers: quantitative variable) against the categorical variables law passed or not (passed:1, not passed:0) and Months. Here the hypothesis is;

Null hypothesis (H0) : The average mean of two groups are not different.
Alternative hypothesis (H1): The average mean of two groups are different.

Using the aov and summary command, we find that p-value is less than 2e-16 for both *law* & *Month.* If we test it against the significance level of 5% (alpha = 0.05) then as p-value < alpha, we **reject the null hypothesis** in favor of alternative hypothesis. Hence the mean of two groups are different which proves that less people died when law of Seat belts was passed. 

The box plot of both classes also shows that the mean value of both groups differ significantly. 

Using webr package, we can plots and get more insight on the data. We can see from Q-Q plot that there are *outliers* on both ends of the chart, but those on the upper end look more severe than those on the bottom and might be slightly right-skewed. But the data is overall fairly normally distributed. The fitted vs Residual plots shows the difference between the expected and actual values.

Adding Month to the model has made the model better as the residuals are now way less and the p-value has also decreased a good amount from one-way anova. This may also explain the seasonal effect in the number of deaths with higher deaths occurring in the Fall/Winter compared to Spring/Summer.


# Regression

Regression can be defined as the parameter to explain the relationship between two separate variables. It is more of a dependent feature where the action of one variable affects the outcome of the other variable. To put in the simplest terms, regression helps identify how variables affect each other.

## Linear Regression Model:

```{r}
# Data set only with entries where law was not passed to test the effect of petrol prices without law passed/not passed affecting the test
Law_notpassed <-filter(Seatbelts,law ==0)

# lm model 
lr <- lm(DriversKilled ~ PetrolPrice, data = Law_notpassed)
summary(lr)

# ggplot of regression 
library(ggplot2)
pvd <- ggplot (Law_notpassed, aes(x = PetrolPrice, y= DriversKilled))+geom_point(size = 2, shape = 18, col="darkgreen") +stat_smooth (method = lm)+ xlab("Petrol Price") + ylab (" Monthly Driver Deaths")
pvd

# f-statistic:
anova(lr)
```

## Multiple Regression Model:

```{r}
# Data set only with entries where law was not passed to test the effect of petrol prices and kms (distance) without law passed/not passed affecting the test
Law_notpassed <-filter(Seatbelts,law ==0)

# linear model 
mr <- lm(DriversKilled ~ PetrolPrice + kms, data = Law_notpassed)
summary(mr)

# save predictions of the model in the new data frame together with variable you want to plot against
predicted_df <- data.frame(Law_notpassed$kms, car_drivers_pred = predict(mr, Law_notpassed))

# predicted line of multiple linear regression
library(ggplot2)
ggplot(data = Law_notpassed, aes(x = kms, y = DriversKilled)) + 
  geom_point(color='blue') +
  stat_smooth(method = lm, color='red',data = predicted_df, aes(x=Law_notpassed$kms, y=car_drivers_pred))

# F-statistic
anova(mr)
```



